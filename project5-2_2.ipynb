{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:50:02.539335Z",
     "start_time": "2025-07-01T17:50:02.537789Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d740967c8444d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:50:02.881867Z",
     "start_time": "2025-07-01T17:50:02.543647Z"
    }
   },
   "outputs": [],
   "source": [
    "# import select functions and types\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import floor, round, monotonically_increasing_id, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import Normalizer, BucketedRandomProjectionLSH\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import UniversalSentenceEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "from pyspark.ml.feature import Normalizer, BucketedRandomProjectionLSH\n",
    "\n",
    "# import modules using an alias\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "GITHUB_URL = \"https://raw.githubusercontent.com/farhodibr/datasets/heads/main/books_recommender/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584cc20ea7f7156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:50:12.901852Z",
     "start_time": "2025-07-01T17:50:02.957866Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"MySparkApp\")\n",
    "conf.set(\"spark.executor.memory\", \"10g\")\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setCheckpointDir(\"checkpoint_dir_als\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a161225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark._jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "#import subprocess\n",
    "#subprocess.run([\"winutils\",\"ls\"], check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94badb39ddde1ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:50:21.738590Z",
     "start_time": "2025-07-01T17:50:12.955581Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "books_file_name = \"Books\"\n",
    "ratings_file_name = \"Ratings\"\n",
    "users_file_name = \"Users\"\n",
    "filenames = [books_file_name, ratings_file_name, users_file_name]\n",
    "\n",
    "for filename in filenames:\n",
    "    download_url = f\"{GITHUB_URL}/{filename}.csv\"\n",
    "    spark.sparkContext.addFile(download_url)\n",
    "\n",
    "ratings_df = spark.read.csv(SparkFiles.get(f\"{ratings_file_name}.csv\"),\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  sep=\",\")\n",
    "\n",
    "books_df = spark.read.csv(SparkFiles.get(f\"{books_file_name}.csv\"),\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  sep=\",\")\n",
    "\n",
    "users_df = spark.read.csv(SparkFiles.get(f\"{users_file_name}.csv\"),\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  sep=\",\")\n",
    "\n",
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30092a16718f3e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:55:34.877249Z",
     "start_time": "2025-07-01T17:55:30.747251Z"
    }
   },
   "outputs": [],
   "source": [
    "# ALS data preparation - ALS good for sparse data\n",
    "\n",
    "users = users_df.select(\"User-ID\").distinct()\n",
    "# We dont wan't the same data/id to be used in different partitions, so just force use 1 partition for this\n",
    "users = users.coalesce(1)\n",
    "users = users.withColumn(\"userIntId\", monotonically_increasing_id()).persist()\n",
    "\n",
    "books = ratings_df.select(\"ISBN\").distinct()\n",
    "# We dont wan't the same data/id to be used in different partitions, so just force use 1 partition for this\n",
    "books = books.coalesce(1)\n",
    "# users_df = users_df.withColumn(\"userIntId\", monotonically_increasing_id()).persist() # use persist to keep these values the same\n",
    "books = books.withColumn(\"bookIntId\", monotonically_increasing_id()).persist()\n",
    "\n",
    "ratings_df_int_ids = ratings_df.join(users, \"User-ID\", \"left\").join(books, \"ISBN\", \"left\")\n",
    "ratings_df_int_ids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734ba9ba56aff14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:55:37.936139Z",
     "start_time": "2025-07-01T17:55:37.508508Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings = ratings_df_int_ids.select(col(\"userIntId\").alias(\"userId\"), col(\"bookIntId\").alias(\"bookId\"), col(\"Book-Rating\").alias(\"rating\"))\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca3f8dafd2a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:55:55.395005Z",
     "start_time": "2025-07-01T17:55:41.313508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the ratings dataframe into training and test data\n",
    "(training_data, test_data) = ratings.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Set the ALS hyperparameters\n",
    "\n",
    "als = ALS(userCol=\"userId\", itemCol=\"bookId\", ratingCol=\"rating\", rank = 10, maxIter = 15, regParam = .1,\n",
    "          coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = False)\n",
    "\n",
    "# Fit the model to the training_data\n",
    "model = als.fit(training_data)\n",
    "\n",
    "# Generate predictions on the test_data\n",
    "test_predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b640e30f38c7a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T17:50:28.738421Z",
     "start_time": "2025-07-01T17:45:36.245470Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e62c69f9e7c6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:21:34.944046Z",
     "start_time": "2025-07-01T17:58:30.282594Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" # Hyperparameter tuning\n",
    "\n",
    "# Takes too long at the moment\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#            .addGrid(als.rank, [10, 50, 75, 100]) \\\n",
    "#            .addGrid(als.maxIter, [5, 50, 100, 200]) \\\n",
    "#            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "#            .build()\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "           .addGrid(als.rank, [10, 50]) \\\n",
    "           .addGrid(als.maxIter, [5, 50]) \\\n",
    "           .addGrid(als.regParam, [.01, .05]) \\\n",
    "           .build()\n",
    "\n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Use cross validation\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Checkpoint the training data to truncate its lineage.\n",
    "# This is a lazy operation, it will be triggered by the .fit() call.\n",
    "training_data_chkp = training_data.checkpoint()\n",
    "\n",
    "# Fit the cross validator on the CHECKPOINTED DataFrame.\n",
    "model = cv.fit(training_data_chkp)\n",
    "\n",
    "# --- MODIFICATION END ---\n",
    "\n",
    "best_model = model.bestModel\n",
    "\n",
    "\n",
    "print (f\"{len(param_grid)} models tested\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f3d7bc16fee18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:32:10.534611Z",
     "start_time": "2025-07-01T18:32:10.361888Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" # Print \"Rank\"\n",
    "print(\"  Rank:\", best_model.rank)\n",
    "# Print \"RegParam\"\n",
    "print(\"  params:\", best_model.params) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64a7eb0a8628b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T18:32:58.417743Z",
     "start_time": "2025-07-01T18:32:47.859582Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" test_data_cached = test_data.cache()\n",
    "test_predictions = best_model.transform(test_data_cached)\n",
    "\n",
    "# Calculate the RMSE of test_predictions\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd11b7c4beb217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T19:16:31.444010Z",
     "start_time": "2025-07-01T19:16:24.310610Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_k_core_subset(data, min_user_ratings=10, min_item_ratings=5):\n",
    "    \"\"\"\n",
    "    Creates a k-core subset of a ratings dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The ratings dataframe with 'user_id' and 'isbn' columns.\n",
    "        min_user_ratings (int): The minimum number of ratings a user must have.\n",
    "        min_item_ratings (int): The minimum number of ratings an item must have.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new dataframe representing the k-core.\n",
    "    \"\"\"\n",
    "    print(\"Starting k-core filtering...\")\n",
    "    k_core_data = data.alias(\"new_df\")\n",
    "\n",
    "    while True:\n",
    "        # Store the original size to check for changes\n",
    "        original_rows = k_core_data.count()\n",
    "        print(f\"Current dataset size: {original_rows} ratings\")\n",
    "\n",
    "        # Filter by minimum item ratings\n",
    "        item_counts= k_core_data.groupBy('ISBN').count()\n",
    "        items_to_keep= item_counts.filter(F.col(\"count\") >= min_item_ratings)\n",
    "        k_core_data =k_core_data.join(items_to_keep.select('ISBN'), 'ISBN', 'inner')\n",
    "\n",
    "        user_counts = k_core_data.groupBy('User-ID').count()\n",
    "        users_to_keep = user_counts.filter(F.col(\"count\") >= min_user_ratings)\n",
    "        # Keep only the User-IDs that are in our users_to_keep dataframe\n",
    "        k_core_data = k_core_data.join(users_to_keep.select('User-ID'), 'User-ID', 'inner')\n",
    "\n",
    "        # Checkpointing breaks the long lineage chain in iterative algorithms\n",
    "        # to prevent StackOverflow errors\n",
    "        k_core_data = k_core_data.checkpoint()\n",
    "\n",
    "        current_rows = k_core_data.count()\n",
    "        # Check if the size has stabilized\n",
    "        if current_rows == original_rows:\n",
    "            print(f\"\\\\nK-core filtering complete. Final dataset size: {current_rows} ratings\")\n",
    "            break\n",
    "\n",
    "    return k_core_data\n",
    "\n",
    "# takes a while\n",
    "# \n",
    "dense_subset = create_k_core_subset(ratings_df_int_ids, min_user_ratings=15, min_item_ratings=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head_tail_slice(ratings_df, n_head_users, n_tail_users, n_head_books, n_tail_books, seed=123):\n",
    "    \"\"\"\n",
    "    Performs head/tail sampling on a Spark DataFrame to create a representative subset.\n",
    "\n",
    "    Args:\n",
    "        ratings_df (DataFrame): The input ratings Spark DataFrame. Must contain 'User-ID' and 'ISBN'.\n",
    "        n_head_users (int): Number of most active users to select.\n",
    "        n_tail_users (int): Number of random less-active users to select.\n",
    "        n_head_books (int): Number of most popular books to select from the user slice.\n",
    "        n_tail_books (int): Number of random less-popular books to select.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The final sampled slice.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Get user counts \n",
    "    user_counts = ratings_df.groupBy(\"User-ID\").count().withColumnRenamed(\"count\", \"user_rating_count\")\n",
    "\n",
    "    # Get the head users (most active)\n",
    "    head_users_df = user_counts.orderBy(F.desc(\"user_rating_count\")).limit(n_head_users)\n",
    "\n",
    "    # Get the tail candidates by finding users not in the head list\n",
    "    tail_candidates_df = user_counts.join(head_users_df, \"User-ID\", \"left_anti\")\n",
    "\n",
    "    # Get the tail users by sampling randomly from the candidates\n",
    "    # We do this by assigning a random number and taking the top N\n",
    "    tail_users_df = tail_candidates_df.withColumn(\"random\", F.rand(seed)).orderBy(\"random\").limit(n_tail_users)\n",
    "\n",
    "    # Combine head and tail users\n",
    "    selected_users_df = head_users_df.select(\"User-ID\").union(tail_users_df.select(\"User-ID\"))\n",
    "\n",
    "    # Restrict ratings to only the selected users (the user slice)\n",
    "    user_slice_df = ratings_df.join(selected_users_df, \"User-ID\", \"inner\")\n",
    "    \n",
    "    # It's a good practice to cache intermediate results that will be reused\n",
    "    user_slice_df.cache()\n",
    "    \n",
    "    # Get book counts from the user_slice\n",
    "    book_counts_sub = user_slice_df.groupBy(\"ISBN\").count().withColumnRenamed(\"count\", \"book_rating_count\")\n",
    "\n",
    "    # Get head books\n",
    "    head_books_sub_df = book_counts_sub.orderBy(F.desc(\"book_rating_count\")).limit(n_head_books)\n",
    "\n",
    "    # Get tail book candidates\n",
    "    tail_candidates_sub_df = book_counts_sub.join(head_books_sub_df, \"ISBN\", \"left_anti\")\n",
    "\n",
    "    # Get tail books\n",
    "    tail_books_sub_df = tail_candidates_sub_df.withColumn(\"random\", F.rand(seed + 1)).orderBy(\"random\").limit(n_tail_books)\n",
    "\n",
    "    # Combine head and tail books\n",
    "    selected_books_df = head_books_sub_df.select(\"ISBN\").union(tail_books_sub_df.select(\"ISBN\"))\n",
    "\n",
    "    # Build the final slice by joining the user_slice with selected books\n",
    "    final_slice_df = user_slice_df.join(selected_books_df, \"ISBN\", \"inner\")\n",
    "\n",
    "    # Clean up the cache\n",
    "    user_slice_df.unpersist()\n",
    "\n",
    "    return final_slice_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will pick 720 users and 800 books to create \"long-tail\" dataset\n",
    "n_head_users, n_tail_users = 300, 800\n",
    "n_head_books, n_tail_books = 300, 800\n",
    "\n",
    "#create the final slice\n",
    "final_slice = create_head_tail_slice(\n",
    "    dense_subset,\n",
    "    n_head_users, n_tail_users,\n",
    "    n_head_books, n_tail_books\n",
    ")\n",
    "\n",
    "final_slice.cache()\n",
    "\n",
    "display(\"Final slice shape:\", (final_slice.count(), len(final_slice.columns)))\n",
    "display(\"Selected users:\", final_slice.select(\"User-ID\").distinct().count())\n",
    "display(\"Selected books:\", final_slice.select('ISBN').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73906710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\nCalculating long-tail distribution...\")\n",
    "bc_df = final_slice.groupBy(\"ISBN\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Collect the result to the driver as a pandas DataFrame\n",
    "bc_pandas_df = bc_df.toPandas()\n",
    "\n",
    "# Now, use the pandas logic for plotting\n",
    "cum = bc_pandas_df['count'].cumsum() / bc_pandas_df['count'].sum()\n",
    "n80 = (cum <= 0.8).sum()\n",
    "\n",
    "#The denominator is the total number of unique books in the slice\n",
    "total_books = len(bc_pandas_df)\n",
    "print(f\"{n80}/{total_books} books cover 80% → {n80/total_books:.1%}\")\n",
    "\n",
    "# Plotting using matplotlib\n",
    "plt.hist(bc_pandas_df['count'].values, bins=20)\n",
    "plt.title(f\"{total_books}-book slice: ratings per book\")\n",
    "plt.xlabel(\"Ratings per book\")\n",
    "plt.ylabel(\"Number of books\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a02677",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/farhodibr/PROJECT5/main\"\n",
    "SINGLE_URL      = f\"{GITHUB_RAW_BASE}/book_metadata/metadata_all.csv\"\n",
    "PARTS_URL_FMT   = f\"{GITHUB_RAW_BASE}/book_metadata_parts/metadata_part_{{}}.csv\"\n",
    "\n",
    "OUTPUT_DIR      = \"book_metadata_parts\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# test if a raw GitHub URL exists \n",
    "def remote_exists(url: str) -> bool:\n",
    "    try:\n",
    "        return requests.head(url, timeout=5).status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "final_schema = StructType([\n",
    "    StructField(\"ISBN\",             StringType(), True),\n",
    "    StructField(\"title\",            StringType(), True),\n",
    "    StructField(\"description\",      StringType(), True),\n",
    "    StructField(\"authors\",          StringType(), True),\n",
    "    StructField(\"publish_date\",     StringType(), True),\n",
    "    StructField(\"publisher\",        StringType(), True),\n",
    "    StructField(\"number_of_pages\",  IntegerType(), True),\n",
    "    StructField(\"subjects\",         StringType(), True)\n",
    "])\n",
    "\n",
    "# Try loading the single combined CSV from GitHub \n",
    "if remote_exists(SINGLE_URL):\n",
    "    print(\"✅ Found metadata_all.csv on GitHub. Loading via pandas → Spark.\")\n",
    "    pdf = pd.read_csv(SINGLE_URL)\n",
    "    # ensure integer pages\n",
    "    pdf['number_of_pages'] = pdf['number_of_pages'].fillna(0).astype(int)\n",
    "    metadata_df = (\n",
    "        spark.createDataFrame(pdf, schema=final_schema)\n",
    "             .cache()\n",
    "    )\n",
    "\n",
    "# Else try loading numbered parts from GitHub \n",
    "elif remote_exists(PARTS_URL_FMT.format(1)):\n",
    "    print(\"found metadata parts on GitHub. Loading all parts via pandas → Spark.\")\n",
    "    parts_urls = []\n",
    "    i = 1\n",
    "    while remote_exists(PARTS_URL_FMT.format(i)):\n",
    "        parts_urls.append(PARTS_URL_FMT.format(i))\n",
    "        i += 1\n",
    "\n",
    "    pdfs = []\n",
    "    for url in parts_urls:\n",
    "        part = pd.read_csv(url)\n",
    "        part['number_of_pages'] = part['number_of_pages'].fillna(0).astype(int)\n",
    "        pdfs.append(part)\n",
    "\n",
    "    all_pdf = pd.concat(pdfs, ignore_index=True).drop_duplicates(\"ISBN\")\n",
    "    metadata_df = (\n",
    "        spark.createDataFrame(all_pdf, schema=final_schema)\n",
    "             .cache()\n",
    "    )\n",
    "\n",
    "#Otherwise fall back to fetching via Google Books API\n",
    "else:\n",
    "    print(\"No GitHub metadata found. Fetching via Google Books API…\")\n",
    "    GOOGLE_API_KEY = \"\"\n",
    "    if not GOOGLE_API_KEY:\n",
    "        raise RuntimeError(\"Please set the GOOGLE_BOOKS_API_KEY env var.\")\n",
    "\n",
    "    def build_url(isbn: str) -> str:\n",
    "        return (\n",
    "            f\"https://www.googleapis.com/books/v1/volumes\"\n",
    "            f\"?q=isbn:{isbn}&key={GOOGLE_API_KEY}\"\n",
    "        )\n",
    "\n",
    "    def fetch_single_book_metadata(isbn, max_retries=3, initial_delay=1):\n",
    "        url   = build_url(isbn)\n",
    "        delay = initial_delay\n",
    "        for attempt in range(1, max_retries+1):\n",
    "            resp = requests.get(url, timeout=15)\n",
    "            if resp.status_code == 429:\n",
    "                ra   = resp.headers.get(\"Retry-After\")\n",
    "                wait = float(ra) if ra else delay\n",
    "                print(f\"[429] {isbn}, retry {attempt}/{max_retries} in {wait}s\")\n",
    "                time.sleep(wait)\n",
    "                delay *= 2\n",
    "                continue\n",
    "            try:\n",
    "                resp.raise_for_status()\n",
    "                data = resp.json()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {isbn} attempt {attempt}: {e}\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "                continue\n",
    "            items = data.get(\"items\")\n",
    "            if not items:\n",
    "                return {\"ISBN\": isbn}\n",
    "            info  = items[0].get(\"volumeInfo\", {})\n",
    "            pages = info.get(\"pageCount\")\n",
    "            try:\n",
    "                pages = int(pages) if pages is not None else 0\n",
    "            except:\n",
    "                pages = 0\n",
    "            time.sleep(0.3)\n",
    "            return {\n",
    "                \"ISBN\":            isbn,\n",
    "                \"title\":           info.get(\"title\", \"\"),\n",
    "                \"description\":     info.get(\"description\", \"\"),\n",
    "                \"authors\":         \", \".join(info.get(\"authors\", [])),\n",
    "                \"publish_date\":    info.get(\"publishedDate\", \"\"),\n",
    "                \"publisher\":       info.get(\"publisher\", \"\"),\n",
    "                \"number_of_pages\": pages,\n",
    "                \"subjects\":        \", \".join(info.get(\"categories\", []))\n",
    "            }\n",
    "        print(f\"[FAILED] ISBN {isbn} after {max_retries} attempts.\")\n",
    "        return {\"ISBN\": isbn, \"number_of_pages\": 0}\n",
    "\n",
    "    def chunker(seq, size):\n",
    "        for i in range(0, len(seq), size):\n",
    "            yield seq[i:i+size]\n",
    "\n",
    "    isbn_list = [r.ISBN for r in final_slice.select(\"ISBN\").distinct().collect()]\n",
    "    batches   = list(chunker(isbn_list, 100))\n",
    "\n",
    "    all_results = []\n",
    "    for idx, batch in enumerate(batches, 1):\n",
    "        print(f\"Batch {idx}/{len(batches)}: fetching {len(batch)} ISBNs…\")\n",
    "        with ThreadPoolExecutor(max_workers=2) as exe:\n",
    "            futures = [exe.submit(fetch_single_book_metadata, isbn) for isbn in batch]\n",
    "            for fut in as_completed(futures):\n",
    "                all_results.append(fut.result())\n",
    "        time.sleep(10)\n",
    "\n",
    "    # save and load via Spark\n",
    "    local_csv = f\"{OUTPUT_DIR}/metadata_all_local.csv\"\n",
    "    pd.DataFrame(all_results).to_csv(local_csv, index=False)\n",
    "    metadata_df = (\n",
    "        spark.read\n",
    "             .schema(final_schema)\n",
    "             .option(\"header\", True)\n",
    "             .csv(local_csv)\n",
    "             .cache()\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Metadata schema:\")\n",
    "metadata_df.printSchema()\n",
    "\n",
    "preview_cols = [\"ISBN\", \"title\", \"authors\", \"publish_date\", \"number_of_pages\"]\n",
    "\n",
    "print(\"Sample metadata (subset of columns):\")\n",
    "print(pdf[preview_cols]\n",
    "      .head(5)\n",
    "      .to_string(index=False))\n",
    "\n",
    "print(\"\\n Description snippet (first 150 chars):\")\n",
    "snippet = pdf[[\"ISBN\", \"description\"]].copy()\n",
    "snippet[\"desc_snippet\"] = snippet[\"description\"].str.slice(0, 150)\n",
    "print(snippet[[\"ISBN\", \"desc_snippet\"]]\n",
    "      .head(5)\n",
    "      .to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c92f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Define a compatible set of versions \n",
    "spark_nlp_version = \"5.3.3\" # A stable version for Spark 3.5.x\n",
    "spark_version_scope = \"3.5\"\n",
    "nlp_package = f\"com.johnsnowlabs.nlp:spark-nlp-spark{spark_version_scope}_2.12:{spark_nlp_version}\"\n",
    "\n",
    "print(f\"Configuring Spark Session to use Spark NLP package: {nlp_package}\")\n",
    "\n",
    "# Stop any existing session to ensure a clean start\n",
    "try:\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "    print(\"Stopped existing Spark session.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Build a new session with the correct Spark NLP package included\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkNLP_Book_Recommender\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", nlp_package) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session configured and started successfully.\")\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import UniversalSentenceEncoder\n",
    "from pyspark.ml.feature import Normalizer, BucketedRandomProjectionLSH\n",
    "\n",
    "print(\"\\n--- Content-Based Recommender Setup ---\")\n",
    "print(f\"Spark NLP version: {sparknlp.version()}\")\n",
    "print(f\"Apache Spark version: {spark.version}\")\n",
    "\n",
    "\n",
    "metadata_path = \"https://raw.githubusercontent.com/farhodibr/PROJECT5/refs/heads/main/book_metadata/metadata_all.csv\"\n",
    "\n",
    "if not os.path.exists(metadata_path):\n",
    "    print(f\"ERROR: '{metadata_path}' not found. Please ensure the file is in the correct directory.\")\n",
    "else:\n",
    "    final_schema = StructType([\n",
    "        StructField(\"ISBN\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"authors\", StringType(), True),\n",
    "        StructField(\"publish_date\", StringType(), True),\n",
    "        StructField(\"publisher\", StringType(), True),\n",
    "        StructField(\"number_of_pages\", IntegerType(), True),\n",
    "        StructField(\"subjects\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    metadata_df = (\n",
    "        spark.read\n",
    "             .schema(final_schema)\n",
    "             .option(\"header\", True)\n",
    "             .csv(metadata_path)\n",
    "             .na.fill({\"title\": \"No Title\", \"description\": \"No Description\"}) # Handle potential nulls\n",
    "             .cache()\n",
    "    )\n",
    "    print(\"✅ Book metadata loaded successfully.\")\n",
    "    metadata_df.select(\"ISBN\", \"title\").show(5, truncate=False)\n",
    "\n",
    "    # We will convert book titles and descriptions into numerical vectors (embeddings).\n",
    "    # Combine title and description into a single feature column\n",
    "    df_for_nlp = metadata_df.withColumn(\n",
    "        \"text\",\n",
    "        F.concat_ws(\" \", F.col(\"title\"), F.col(\"description\"))\n",
    "    ).filter(F.col(\"text\") != \"No Title No Description\")\n",
    "\n",
    "    # Define the Spark NLP pipeline stages\n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(\"text\") \\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "    use = UniversalSentenceEncoder.pretrained() \\\n",
    "        .setInputCols([\"document\"]) \\\n",
    "        .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "    finisher = Finisher() \\\n",
    "        .setInputCols([\"sentence_embeddings\"]) \\\n",
    "        .setOutputCols([\"finished_embeddings\"]) \\\n",
    "        .setOutputAsVector(True)\n",
    "\n",
    "    nlp_pipeline = Pipeline(stages=[document_assembler, use, finisher])\n",
    "\n",
    "    # Generate the embeddings\n",
    "    print(\"\\nGenerating book embeddings with Spark NLP (this may take a few minutes)...\")\n",
    "    embeddings_df = nlp_pipeline.fit(df_for_nlp).transform(df_for_nlp)\n",
    "    \n",
    "    # Extract the vector from the finished array\n",
    "    embeddings_df = embeddings_df.withColumn(\"embedding\", F.col(\"finished_embeddings\")[0])\n",
    "    print(\"✅ Embeddings generated.\")\n",
    "\n",
    "\n",
    "    # Use LSH to Find Similar Books \n",
    "    # Locality Sensitive Hashing (LSH) is an efficient way to find nearest neighbors.\n",
    "    \n",
    "    # Normalize embeddings for LSH\n",
    "    normalizer = Normalizer(inputCol=\"embedding\", outputCol=\"norm_embedding\", p=2.0)\n",
    "    normalized_df = normalizer.transform(embeddings_df)\n",
    "\n",
    "    # Create and train the LSH model\n",
    "    brp = BucketedRandomProjectionLSH(\n",
    "        inputCol=\"norm_embedding\",\n",
    "        outputCol=\"hashes\",\n",
    "        bucketLength=2.0,\n",
    "        numHashTables=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining Locality Sensitive Hashing (LSH) model...\")\n",
    "    lsh_model = brp.fit(normalized_df)\n",
    "    print(\"✅ LSH model trained.\")\n",
    "\n",
    "\n",
    "    # Create the Recommendation Function \n",
    "    # This function takes a book title and finds similar books.\n",
    "\n",
    "    def get_content_based_recs(book_title: str, num_recs: int = 10) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Finds and returns content-based book recommendations for a given book title.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Finding recommendations for: '{book_title}'\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        try:\n",
    "            # Find the embedding vector for the input book\n",
    "            book_embedding = normalized_df.filter(F.col(\"title\") == book_title).first()[\"norm_embedding\"]\n",
    "        except (TypeError, IndexError):\n",
    "            print(f\"Could not find the book '{book_title}' in the dataset.\")\n",
    "            return spark.createDataFrame([], schema=\"title STRING, authors STRING, distance FLOAT\")\n",
    "\n",
    "        # Use the LSH model to find the most similar items\n",
    "        print(f\"Searching for the top {num_recs} most similar books...\")\n",
    "        recommendations_df = lsh_model.approxNearestNeighbors(normalized_df, book_embedding, num_recs + 1)\n",
    "\n",
    "        # Format and display the results\n",
    "        recs = recommendations_df.select(\"title\", \"authors\", F.col(\"distCol\").alias(\"distance\")) \\\n",
    "                                 .filter(F.col(\"title\") != book_title) \\\n",
    "                                 .orderBy(\"distance\")\n",
    "        \n",
    "        if recs.count() == 0:\n",
    "            print(\"No similar books found.\")\n",
    "        else:\n",
    "            print(\"\\n--- Recommendations ---\")\n",
    "            recs.show(num_recs, truncate=False)\n",
    "        \n",
    "        return recs\n",
    "\n",
    "    # -\n",
    "    # test content-based recommender directly.\n",
    "    lost_world_recs = get_content_based_recs(\"The Lost World\")\n",
    "    beach_recs = get_content_based_recs(\"The Beach\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
